[
  {
    "objectID": "MSBA_modules_overview.html",
    "href": "MSBA_modules_overview.html",
    "title": "Overview",
    "section": "",
    "text": "Text here and here."
  },
  {
    "objectID": "module_prepayment_modeling.html",
    "href": "module_prepayment_modeling.html",
    "title": "Pre-Payment Modeling",
    "section": "",
    "text": "Learnig objectives\nMain business question\nCase detailed questions\nMapping to Ravi’s coure week: 3 (Classification Week)\nOther notes"
  },
  {
    "objectID": "module_loss_severity_summary.html",
    "href": "module_loss_severity_summary.html",
    "title": "Loss/Severity To-Date Summary Statistics",
    "section": "",
    "text": "Learnig objectives\nMain business question\nCase detailed questions\n\nWhat is your hypothesis about factors predicting “ZBC”\nWhat explains loan volume variations across years (e.g. 2000-2005)\n\nMapping to Ravi’s coure week: 2\nOther notes: Divide into basic; geo; unsupervised"
  },
  {
    "objectID": "module_LLPA_evaluation.html",
    "href": "module_LLPA_evaluation.html",
    "title": "Identifying LLPA Matrix Opportunties",
    "section": "",
    "text": "Learnig objectives\nMain business question\nCase detailed questions\nMapping to Ravi’s coure week: 5 (Prescriptive Week)\nOther notes"
  },
  {
    "objectID": "module_dynamic_prediction.html",
    "href": "module_dynamic_prediction.html",
    "title": "Loan-level dynamic outcome prediction",
    "section": "",
    "text": "Learnig objectives\nMain business question\nCase detailed questions\nMapping to Ravi’s coure week: 4 (Regression Week)\nOther notes"
  },
  {
    "objectID": "module_communication.html",
    "href": "module_communication.html",
    "title": "Communicating Recommendations",
    "section": "",
    "text": "Learnig objectives\nMain business question\nCase detailed questions\nMapping to Ravi’s coure week\nOther notes"
  },
  {
    "objectID": "module_acquisition_summary.html",
    "href": "module_acquisition_summary.html",
    "title": "Exploratory Analytics: Exploring Acquisition Characteristics",
    "section": "",
    "text": "The main acquisition characteristics considered to in this module are:\n- Original Interest Rate\n- Original UPB\n- Original Loan to Value Ratio (LTV)\n- Borrower Credit Score at Origination\n- Debt-To-Income (DTI) \n- Loan Purpose\n- Property Type\nOther acquisition characteristics, also worth of exploration, include: Original Combined Loan to Value Ratio (CLTV), Co-Borrower Credit Score at Origination, Mortgage Insurance Type, Mortgage Insurance Percentage, Channel, Number of Borrowers, Number of Units, Occupancy Status, First Time Home Buyer Indicator, Relocation Mortgage Indicator, and Special Eligibility Program.\nExamine the questions below using the data sample of 30 year fixed-rate mortgages acquired by Fannie Mae during the period 2000-2014. The\n\nThe sub-prime mortgage crises (2007-2009) was precipitated by a rise in sub-prime mortgage lending in the preceding few years. Explore the dataset to identify any marked changes in mortgage acquisition characteristics before and after the crisis period.\nThe interest rate of 30-year fixed-rate mortgages changes over time according to various economic conditions. The changes have significant implications for home buyers and for the mortgage industry as a whole. One record of historical mortgage interest rates is the The Primary Mortgage Market Survey (PMMS) conducted by Freddie Mac, a government sponsored enterprise similar to Fannie Mae, since 1970. Using annual averages, compare the 30-year fixed-rate interest rates with the average interest rates in the Fannie Mae acquisition dataset. Are there any discrepancies between the two time series?1\nExplain carefully the increase in the number of loans in the period 2000-2003 in terms of changes of interest rates during that period.\nWhich States / Zip Codes had the highest/lowest mortgage interest rates? Which States / Zip Codes had the highest/lowest mortgage principal balances at origination?\nComment on whether the mortgages in the dataset conformed strictly to the Eligibility Matrix."
  },
  {
    "objectID": "module_acquisition_summary.html#footnotes",
    "href": "module_acquisition_summary.html#footnotes",
    "title": "Exploratory Analytics: Exploring Acquisition Characteristics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFreddie Mac has recently decided to base their PMMS on data collected from loan applications submitted to Freddie Mac instead of conducting an actual survey…↩︎"
  },
  {
    "objectID": "FNMA_resources.html#i.-expository-notes",
    "href": "FNMA_resources.html#i.-expository-notes",
    "title": "Case Outline",
    "section": "I. Expository Notes",
    "text": "I. Expository Notes\n\nNote 1: The Home Mortgage Market Supply Chain in the U.S.\n\n\nNote 2: Recognizing and Overcoming the Challenges of Manipulating Large Datasets\n\n\nNote 3: The Pricing and Execution of Whole Loans by Fannie Mae\n\n\nNote 4: Quantifying the Porfitability of Whole Loans"
  },
  {
    "objectID": "FNMA_resources.html#ii.-case-modules",
    "href": "FNMA_resources.html#ii.-case-modules",
    "title": "Case Outline",
    "section": "II. Case Modules",
    "text": "II. Case Modules\n\nCase A: Manipulating Modern Business Scale Datasets\n\nLearnig objectives\nMain business question\nCase detailed questions\nMapping to Ravi’s coure week: 1\nOther notes\n\n\n\nCase B: Exploratory Data Analysis\n\nLearnig objectives\nMain business question\nCase detailed questions\n\nWhat is your hypothesis about factors predicting “ZBC”\nWhat explains loan volume variations across years (e.g. 2000-2005)\n\nMapping to Ravi’s coure week: 2\nOther notes: Divide into basic; geo; unsupervised\n\n\n\nCase C: Estimate Loan Profit/Loss to-Date\n\nLearnig objectives\nMain business question\nCase detailed questions\nMapping to Ravi’s coure week: 2\nOther notes\n\n\n\nCase D1: Classifiy Loan Outcome at Origination\n\nLearnig objectives\nMain business question\nCase detailed questions\nMapping to Ravi’s coure week: 3 (Classification Week)\nOther notes\n\n\n\nCase D2: Classifiy Loan Outcome at Interim Dates\n\nLearnig objectives\nMain business question\nCase detailed questions\nMapping to Ravi’s coure week: 3 (Classification Week)\nOther notes\n\n\n\nCase E1: Predict Loan Profit/Loss at Origination\n\nLearnig objectives\nMain business question\nCase detailed questions\nMapping to Ravi’s coure week: 4 (Regression Week)\nOther notes\n\n\n\nCase E2: Predict Loan Profit/Loss at Interim Dates\n\nLearnig objectives\nMain business question\nCase detailed questions\nMapping to Ravi’s coure week: 4 (Regression Week)\nOther notes\n\n\n\nCase F: Assess Validity of LLPA Matrix\n\nLearnig objectives\nMain business question\nCase detailed questions\nMapping to Ravi’s coure week: 5 (Prescriptive Week)\nOther notes\n\n\n\nCase G: Design Your LLPA Matrix\n\nLearnig objectives\nMain business question\nCase detailed questions\nMapping to Ravi’s coure week: 5 (Prescriptive Week)\nOther notes\n\n\n\nCase H: Examine Georgaphical Variations\n\nLearnig objectives\nMain business question\nCase detailed questions\nMapping to Ravi’s coure week\nOther notes\n\n\n\nCase I: Prescriptive Analytics\n\nLearnig objectives\nMain business question\nCase detailed questions\nMapping to Ravi’s coure week\nOther notes"
  },
  {
    "objectID": "FNMA_resources.html#iii.-resources",
    "href": "FNMA_resources.html#iii.-resources",
    "title": "Case Outline",
    "section": "III. Resources",
    "text": "III. Resources\n\nResource Collection 1: FNMA Public Resources\n\n\nResource Collection 2: Python Resources"
  },
  {
    "objectID": "FNMA_resources.html#iv.-synthetic-datasets",
    "href": "FNMA_resources.html#iv.-synthetic-datasets",
    "title": "Case Outline",
    "section": "IV. Synthetic Datasets",
    "text": "IV. Synthetic Datasets"
  },
  {
    "objectID": "FNMA_resources.html#v.-instructor-resources",
    "href": "FNMA_resources.html#v.-instructor-resources",
    "title": "Case Outline",
    "section": "V. Instructor Resources",
    "text": "V. Instructor Resources"
  },
  {
    "objectID": "expository_note_large_data.html",
    "href": "expository_note_large_data.html",
    "title": "Working with Large Datasets",
    "section": "",
    "text": "Analytics-savvy managers and business analysts recognize and are equipped to handle the challenges inherent in working with large datasets. Importing, manipulating, and pre-processing data for effective analysis is typically one of the most time and resource consuming tasks of the analytics pipeline. It is also one that is higly error-prone and costly to fix, for errors ‘baked’ into data cannot always be uncovered during analysis. You can expect the data you’ll encounter professionally to keep growing in volume (records, attributes, and files), variety of formats, and velocity of updates. As a rough guideline, you can consider datasets / dataframes exceeding 1GB in size to be “large”1 because such datasets can pose computing time and memory challenges on currently available personal computers.\nOne learning objective of this case is to gain familiarity in working with large structured datasets. The median size of the quarterly datasets posted by Fannie Mae is approximately 400-500 MB in compressed format2. Here are a few general guidelines:"
  },
  {
    "objectID": "expository_note_large_data.html#footnotes",
    "href": "expository_note_large_data.html#footnotes",
    "title": "Working with Large Datasets",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n‘Big data’, whose size exceeds, say, 50GB requires specialized and cloud-based tools beyond the scope of this case↩︎\nThe compression ratio is quite high; 15:1 or higher↩︎\nDifferent columns can have different data types↩︎\nFor example, if a column representing day of week, say, is coded 1-7, you would need to define this column as a string or categorical data type for analysis purposes. Python or R cannot infer a column’s meaning↩︎\nDate types are especially useful when analyzing time series↩︎\nThink of sorting, for example↩︎"
  },
  {
    "objectID": "expository_mortgage_supply_chain.html",
    "href": "expository_mortgage_supply_chain.html",
    "title": "The U.S. Mortgage Supply Chain",
    "section": "",
    "text": "A home mortgage is a loan associated with the purchase or refinance of a real estate dwelling where the value of the underlying real estate is used as collateral in the case of default.\nSize of the U.S. mortgage market?\nMany types of mortgages exist with varying parameter values. In this case, it sufficies to focus on conventional 30-year fixed rate mortgages. There a financial institution (bank) or broker lends a borrower a fraction of the home value (typically not exceeding 80%) with the expectation that the borrower will repay the principal plus interest (specified by the note rate) in equal monthly installments over 360 months.The borrower has the option to pre-pay the loan’s remaining principal with no penalty at any time before the term limit.\nPlayers: Lender (originates the loan )\nServicing the loan for a fee\nGovernment agencies designed to improve efficiency of the mortgage market and also to promote home ownership among American households.\nGovernment agencies: FNMA, FHLC, GNMA\n\na nation-wide secondary mortgage market\n\n\nlevel-payment fixed-rate mortgage\n\nservicing fee is typically a fixed percentage of the unpaid principle balance and is deducted from the interest payment\nmaturity\nPSA Standard Prepayment Benchmark"
  },
  {
    "objectID": "expository.html",
    "href": "expository.html",
    "title": "Expository Notes",
    "section": "",
    "text": "In order to apply Analytics effectively and responsibly in a business setting, you need a deep understanding of problem context, a solid grasp of relevant methodologies, and skill in implementation tools, and the data. We provide here a collection of expository notes that help you get started on some of these components."
  },
  {
    "objectID": "expository.html#section",
    "href": "expository.html#section",
    "title": "Expository Notes",
    "section": "Section",
    "text": "Section"
  },
  {
    "objectID": "expository.html#i.-problem-context",
    "href": "expository.html#i.-problem-context",
    "title": "Expository Notes",
    "section": "I. Problem Context",
    "text": "I. Problem Context\n\nThe U.S. Home Mortgage Supply Chain\n\n\nThe Pricing and Execution of Whole Loans by Fannie Mae\n\n\nQuantifying the Profitability of Whole Loans"
  },
  {
    "objectID": "expository.html#ii.-methodological",
    "href": "expository.html#ii.-methodological",
    "title": "Expository Notes",
    "section": "II. Methodological",
    "text": "II. Methodological\n\nManipulating Large Datasets\nRecognizing and Overcoming the Challenges of Manipulating Large Datasets (say ~ &gt; 1 GB)\nOn a personal computer or laptop\nResources: Storage, Memory, and Time\nDoubling the number of rows in a dataframe more than doubles the time it takes to sort it."
  },
  {
    "objectID": "expository.html#iii.-tools",
    "href": "expository.html#iii.-tools",
    "title": "Expository Notes",
    "section": "III. Tools",
    "text": "III. Tools"
  },
  {
    "objectID": "data_train_test.html",
    "href": "data_train_test.html",
    "title": "A Train / Test Split",
    "section": "",
    "text": "Description\nIn this final collection of consolidated-loan datasets, we construct a train / test split as follows. We assume the analyst is training (i.e. fitting) their models on data from up to Year 2014 (the first 15 years of acquisition data) and validating / testing their models on data from Year 2015 onwards. Therefore, we construct a train set by considering the loans in the random sample discussed in Section [data_all_years_sample.ipynb] that were acquired prior to Year 2015. For these loans, however, the consolidation of loan data is done with respect to their histories up to the end of Year 2014 only. That is, we set the values of their dynamic fields to those that existed on December 2014 or when the loan reached zero balance whichever was earlier. This strictly ensures no ‘peeking ahead’ in time. For the test / validation set, we consider the loans in the random sample that were acquired in Year 2015 onward. The test set loans are consolidated using their full histories (that is up to the end of Quarter 1 of Year 2023). These train and test sets are provided in several file formats.\n\n\nTrain set\nThis train dataset consists of 2,244,409 loans.\n\nDownload delimited &lt;.csv&gt; flat file (delimiter: “|”)\n\nUncompressed (~585MB) filename: &lt;train_sample_consolidated.csv&gt;\nCompressed (~125MB) filename: &lt;train_sample_consolidated_csv.zip&gt;\n\n\n\nDownload feather &lt;.feather&gt; file\n\nUncompressed (~285MB) filename: &lt;train_sample_consolidated.feather&gt;\nCompressed (~140MB) filename: &lt;train_sample_consolidated_feather.zip&gt;\n\n\n\n\nTest set\nThis test dataset consists of 1,653,349 loans.\n\nDownload delimited &lt;.csv&gt; flat file (delimiter: “|”)\n\nUncompressed (~500MB) filename: &lt;test_sample_consolidated.csv&gt;\nCompressed (~100MB) filename: &lt;test_sample_consolidated_csv.zip&gt;\n\n\n\nDownload feather &lt;.feather&gt; file\n\nUncompressed (~225MB) filename: &lt;test_sample_consolidated.feather&gt;\nCompressed (~115MB) filename: &lt;test_sample_consolidated_feather.zip&gt;"
  },
  {
    "objectID": "data_overview.html",
    "href": "data_overview.html",
    "title": "Overview",
    "section": "",
    "text": "Extensive data on a portion of the mortgages acquired by Fannie Mae since year 2000 is accssible through Fannie Mae’s Data Dynamics portal. The first subsection below helps you get started with this dataset. The second section highlights the need to consolidate the data, which tracks each loan’s performance each month, into a one row per loan summary. The remaining sections provide samples of such consolidated loan data derived from Fannie Mae’s data.\n\nFannie Mae’s Data\nData Consolidation\nA One Year Sample\nA 10% Sample Covering All Years\nA Train / Test Split\n\nIf you need to work with the most detailed and recent mortgage data or if you wish to gain experience in manipulating large datasets then your best starting point is Fannie Mae’s data portal itself followed by Case Module 0: Data Pre-processing. However, if you wish to by-pass much of the data prep work then you will find that the consolidated data samples we have included here provide convenient on-ramps into your analysis."
  },
  {
    "objectID": "data_consolidation.html",
    "href": "data_consolidation.html",
    "title": "Data Consolidation",
    "section": "",
    "text": "Why Consolidate?\nFor the most part, the questions addressed in this case do not require a detailed examination of the monthly time-series of each loan. It sufficies to work with a consolidated dataset, consisting of one row per loan, with fields capturing only the acquisition characteristics and the final or current status of each loan. This consolidation requires some initial pre-processing of the raw data and entails the loss of some detail, but it has the advantage of vastly reducing the size (number of rows) of the datasets required for analysis.\n\n\nHow to Consolidate Mortgage Level Data\nFannie Mae refers to this loan consolidation process in their Loan Performance Data Tutorial and provides, a perhaps somewhat outdated, R Code (Primary dataset) to consolidate the data.\nIt is instructive for students with sufficient time and computing resources to write their own code to consolidate the mortgage loans. We recommend Python, especially the aggregation functionality of pandas. The process is conceptually simple: For each loan, extract the earliest available data for static fields and the latest available data for dynamic fields. See this section for a definition of the static and dynamic fields. Alternatively, we provide samples of consolidated datasts in the next sections."
  },
  {
    "objectID": "case_v0.html",
    "href": "case_v0.html",
    "title": "Expository Notes",
    "section": "",
    "text": "R. Ravi\nTepper School of Business, Carnegie Mellon University\nv2, 8 July 2019\n\n\nFannie Mae (FNMA) is a U.S. government sponsored enterprise that was founded in 1938 during the great depression with the purpose of expanding the secondary mortgage market. Along with Freddie Mac (FHLMC) that was created in 1970 to buy mortgages in the secondary market and pool them to sell as mortgage-backed securities, Fannie Mae helps lenders re-invest their assets and hence increase the lenders in the market. In this way, these two organizations reduce the reliance on local associations and create a more liquid market for mortgage-indexed debt.\nThe underlying mechanism for Fannie Mae to buy loans from smaller local banks that originate mortgages is to specify a set of conforming mortgage parameters - these are re- quirements that the loans should meet to provide an acceptable level of risk for purchase by Fannie Mae. They typically include conditions such as the following: the LTV (loan-to- value) ratio of the loan should be small, which signals that the borrower has at least come up with sufficient funds otherwise that will be liable for collection in case of default2.\nThe risk profile of a typical fixed-rate 30-year mortgage consists of two components: credit risk and interest rate risk. The former consists of the default risk where the borrower stops paying back the monthly installments turning the loan into delinquent state and moving it towards collection. The interest rate risk is associated with varying (U.S. Treasury’s) interest rates for investing money in risk-less deposits over the future term of the loan, which in turn is highly correlated with prevailing mortgage rates. An increase in interest rates will make the loans more profitable while a decrease will potentially lead more borrowers to refinance. Thus, this risk includes the additional optionality risk of the borrower refinancing the loan and prematurely terminating the stream of loan payments by prepayment. The business model of Fannie Mae is to purchase loans from originating banks so as to de-risk the credit risk associated with the loan; the purchased loans are then bundled into mortgage-backed securities that do not suffer credit risk but only the residual interest rate risk3.\nOne of the key mechanisms by which Fannie Mae is able to enable a liquid market for loans is to provide a “cash window” before the closing of every mortgage in which it is open to buy these mortgages at pre-determined rates. For mortgages that have been lent based on the conforming mortgage parameters and hence eligible for purchase by Fannie Mae as determined in their eligibility matrix, during the cash window, Fannie is open to purchasing them. When it buys these loans, it pays originators based on the features of the loan (such as the LTV or loan-to-value ratio, and borrower characteristics such as FICO score). The value at which the loans will be purchased are based on a baseline value per $100,000 along with adjustments primarily based on the LTV and borrower’s FICO score, as well as other features of the mortgage such as if the purpose was COR (Cash-out refinance), and whether the borrower obtained Subordinate Financing or Minimum Mortgage Insurance Coverage for the loan.\n\n\nThe adjustments to the baseline price are detailed in a loan-level price adjustment (LLPA) matrix that is published by Fannie Mae and sporadically updated based on market conditions. These price adjustments are cumulative in that a mortgage that has purpose COR and obtains Minimum Mortgage insurance coverage will pay the sum of the LLPA adjustments for these two features in the price. As an example, consider Example 1b from Page 6 of the December 2010 LLPA matrix published with this case. The 30-year fixed rate mortgage for the purpose of Cash-Out Refinancing (COR) by a borrower with credit score 680 on a loan with LTV (loan-to-value) ratio of 85% and obtaining Minimum Mortgage Insurance (MI) and originating in May 2011 has the following adjustments: 0.25% for the basic Adverse Market Delivery Charge (AMDC) of taking up the mortgage4, the credit score and LTV combination of the borrower adds 1.5% (from Table 2 bottom half for loans originating after April 1, 2011) the COR purpose for a loan with this LTV-FICO combination adds 2.5% (Table 3 bottom half) and finally the MI adds 0.125% (Table 5) for a total of 4.375% to the price of the mortgage.\nMortgage originators (like banks and brokers) use the LLPA matrix in the following way. First they obtain a pricing matrix that is published by Fannie Mae based on the prevailing mortgage rate for 30-year fixed rate mortgages (See the example june2019pricing.pdf that prevailed around June 15th 2019 provided with this case). This matrix details the discounts or premiums that Fannie will pay in terms of the note rate (the annual interest rate at which the loan is closed) and the time window within which the closing will occur (thus, varying durations for the cash window). Intuitively, notes with higher rates and closed closer to the current day will fetch higher premiums. It is important to note that these prices are in basis points which are simply the percentage of the prices. Thus, the pricing premium of 5.6845 points for a loan with note rate of 5.875% to be closed within 5 days means that a loan for 100,000 USD with these parameters will fetch the originator of the mortgage 105,685 USD by selling to Fannie. Typically the note rate at which the premium or discount is at zero roughly corresponds that day’s 30-year fixed rate announced by Fannie and available as a historical record here.\nConsider an example of a loan originator who is trying to determine the best rate to offer a borrower by studying the prevailing pricing matrix, the LLPA matrix and her own costs and margins. Suppose a loan for 100,000 USD is to be closed on 18 June 2019, and the pricing when the loan is to be locked occurs on June 15th when the Fannie rate is between 3.375% and 3.5% and hence reflected in the enclosed june2019pricing.pdf document. Also, suppose the sum of the costs incurred by the originator plus the margin she would like to earn on closing the loan total 2,000 USD and hence 2 points. Furthermore, suppose the loan is taken out for the purpose of Cash-Out Refinancing (COR) by a borrower with credit score 690 on a loan with LTV (loan-to-value) ratio of 80%: the LTV-credit score combination in the LLPA matrix adds 1.75 points and the COR purpose adds another 1.75 points for a total of 3.5 points. Thus the originator needs to make 2 + 3:5 = 5:5 points above par to sell the loan to Fannie. This involves looking up the june2019pricing.pdf table again for the note rate that is closest to fetching 5.5 points above par (i.e., 105,500 USD), which in this case corresponds to a 5.875% final interest rate for the customer. When Fannie buys such a loan, it pays the originator 5.6845 (from the pricing matrix) minus the 3.5 points LLPA adjustment and hence a total of 102,185 USD, which in turn will cover the 2 points expectation for the costs and margins of the originator.\n\n\n\nTo reconcile if Fannie’s purchases made better or worse than what they were purchased for, you should ideally simulate for each loan purchased by Fannie, the price it actually paid using the pricing matrix based on the note rate and closing window, and examine the stream of cash flows it received regarding this loan (including default costs if any). However, these prices Fannie paid are not available publicly. Moreover, each of the originators may have different costs and also seek different margins which would further obscure the calculation, since we will be unable to impute this addition that they use in computing the final interest rate in the originating process.\nTo avoid the above two data difficulties, you can simplify the problem and just examine whether the LLPA price adjustments of Fannie are correctly benchmarked against the baseline set of loans on which these LLPA adjustments are zero. To do this, you can look at the additional default losses suffered by Fannie in each of the LLPA adjustment buckets other than a set of baseline groups (with zero LLPA adjustment) and examine how the net losses from one of these groups compares with the aggregate adjustment fee collected by Fannie on that group.\n\n\n\n\nThe network of Federal Home Loan banks were established as a cooperative of local banks and lending institutions within a region to further increase liquidity in the mortgage markets. For the purpose of this case, assume that you are an Analytics Director of the Federal Home Loan bank of your region (which will be assigned based on the group that you are part of). Your goal is to design new schemes to improve the profitability of your bank without negatively impacting the mandate of increasing and enabling the liquidity of the markets for mortgages. This typically involves offering more attractive pricing schemes than Fannie in buying back loans from your local originators.\n\n\nYou do not have any report to turn in the first week. You can take the time to read the case and understand the business context as you read the case.\n\n\n\nFannie Mae has made a portion of the data about the 30-year fixed rate loans it has purchased since 2000 and their performance over time available publicly here. To access this, you will first need to register at this website with a valid email address and basic information.\nTo familiarize yourself with the data there are two tutorials in this main information website, along with an overview as well as sample code in R and SAS to import and combine the two data tables as explained in the tutorial. Your first task for the first week will be to work through the first tutorial and build a flat combined data file5. You can then refine it for the specific subsamples of interest for your team (city, metropolitan statistical areas/MSA or state) later by filtering the full data using a subset of three-digit ZIP codes and/or MSA’s. You second task will be to build informative sample statistics from the data set to familiarize yourself with its characteristics.\nWe will examine all the loans purchased in 2010 in or around four metropolitan areas: Atlanta, Dallas, New York and San Francisco. Based on the team you are in, you will work with one of these areas in more detail. In choosing which subset of loans you want to analyze as a team in your region, you may want to assemble three levels of data sets: First, you can assemble a set for a couple of three-digit ZIP codes that define a specific city in your region (e.g., Atlanta - 303 and 311, Dallas - 752 and 753, New York - 100, 101 and 102, San Francisco - 941). At the next level you can find the appropriate MSA’s that encompass a slightly larger area. Finally, you can simply gather the data set for the whole state using three-digit ZIP code refinements from the information available here.\nFor Week 2, as a group you can first follow the steps in the tutorial for the whole data set of acquisitions from 2010. Note that in addition to downloading the acquisition data files for the four quarters of 2010, you will need to download the performance data for all quarters for all years from and including 2010. Plan to allocate enough time to download the data - if you have enough space in your machine, it may be more convenient to download all the files since 2000 as one big download (nearly 25 GB).\nAs your submission for the second week, I would like you to carry out an analysis similar to what is given in the webinar-101.pdf in the Fannie Mae website. Note that they also detail the data cleaning choices they made and how the performance files over time have been combined into a single data file with one row per loan. You will find that watching the accompanying online tutorial will clarify a lot of the computations. Repeat this computation in your favorite computing environment for all loans acquired in 2010, and for the subsets in your regions (city, MSA, state) to spot any variations.\nSince we will eventually be concerned with whether the price adjustments are accurate and try to identify opportunities from any deviations, there are two measures you can pay particular attention to in your analysis. The first is the loss suffered from a set of mortgages as a percentage of the total original principal - the reason why this will be important is because the difference in this measure between a baseline group of loans (with zero LLPA adjustment) and a group with a specific adjustment will give us an estimate of the additional default cost to Fannie of serving the second group against the baseline. This can then directly be compared with the price adjustment in the LLPA matrix which is also in percentages. The Net Loss Rate calculation in the first tutorial will give you a good starting point to do this. The other measure you will eventually need to grapple with is future defaults from currently active loans. For this, it may be useful to find simple summaries and visualizations of the default behavior of set of loans that have been active for 5 or 10 years, and how they evolve over time. Other than these two speciic concerns, you can and should be open to finding interesting summaries and descriptive patterns in the data at this early stage to deepen your understanding of the data in the case context.\n\n\n\nYou should begin assembling the datasets for the loans in your regions this week if you did not already in the previous. Your task this week is to determine how the loans serviced by Fannie Mae performed in their portfolio. In other words, your goal is to assess the quality of Fannie’s LLPA adjustments.\nFor this, we will work with the set of loans in your region purchased by Fannie during 2010. As an example, let us consider a specific risk cohort which is the subset of loans with primary borrower FICO score range 660-679 and LTV range 75.01-80% having a LLPA adjustment of 2.5%, which is one of the primary risk buckets in the December 2010 LLPA matrix with positive LLPA. Note that all loans have a AMDC of 0.25% which when added to the negative LLPA adjustments in the first column of Table 2 gives a 0% adjustment. Thus we can use as a baseline group the set of loans with primary borrower FICO scores 700 or above and LTV up to 60%.\nFor each of these different risk cohorts,, you will need to determine the default losses as a percentage of the original principal. This is also termed the Net Loss Rate in the tutorials. The calculation is relatively direct: Using the combined data set from the first week, filter out the group of loans of interest (say baseline or a specific risk cohort). For all the loans taken out in 2010 in the group, compute the net loss for the group as a percentage of the total principal.\nNote that for the loans that are still active from the portfolio, you still do not know their future performance. For this week, you may try to build a simple summary average of the percent of the original loan value of a cohort that defaults in each of the first through later years. Next week, you can try to refine this by trying to build a model for predicting the percentage of default in each of the years in the lifetime of the loan. For the simple summary, you will need to collect loans of your risk cohort over as many years as you can in the past, and build a table of the fraction of the original principal that defaulted each of the years that the loan has been active. For those originating in 2000, you have nearly 18 years of such data whereas for the cohort beginning in 2015, you will only have this for the first three years. Overlaying these summaries of default percentage across the life of the loan and averaging, you can get a measure of the default loss as a percentage of the principal in each year for each cohort6. You can then use these summaries to extrapolate the losses on the loans from 2010 for the next 20 years (You will still need to make some assumptions about the losses in the last ten years of the loan for which you will have no data). Whatever you do, please be very clear in your weekly update about the assumptions you made in carrying out this audit calculation.\nRedo the loss differential calculation across different risk cohorts based on the LTV and FICO ranges, as well as other potential risk factors like Cash Out Refinancing as loan purpose and Investment Property, which also have corresponding LLPA adjustments. Do this across the whole data set as well as the different levels in your regions to find any significant deviations. At the end of this week, you will prepare a report summarizing your analysis of the profit/loss accrued to Fannie as a result of its pricing, and any opportunity this presents to your own Federal Home Loan bank to offer intermediation based on any geographic variation you identified.\n\n\n\nThe next step of the analysis is a prelude to help you design new products to offer your member banks. At a minimum, you may attempt to buy loans from your local originators (assume all of them to be member banks of your organization) by providing them a more competitive price for the loans they originate than is reflected in Fannie’s pricing. However, to be able to do this effectively, you will either need more information for risk arbitrage than what is available to Fannie, or you will need a way to cover default losses more reliably than Fannie is able to. In other words, you should either be able to get a better estimate than Fannie of the credit worthiness of the borrowers in your region whose loans you propose to buy, or you should have the potential for a risk-sharing agreement with your local banks to mitigate any risks that might arise from your better pricing. Both are plausible avenues to investigate, and let us call them loosely “risk refinement” and “risk sharing” respectively.\nFor the former (risk refinement), you may be able to find and use additional data about specific metropolitan areas that might aid in your assessment of property value evolution which in turn may make some loans more or less advantageous due to the resulting changes in their LTV. For the latter (risk sharing), since the local banks are the actual shareholders and hence owners of your bank, they may be amenable to an agreement where your more competitive pricing is accompanied by a credit buffer that you set up with them that will be a first pool of resource to turn to in the eventuality of loss events from that bank’s portfolio. This buffer can be adjusted in size and depletion rules based on the performance of the originated loans from that bank.\nFor the case, I would like you to propose a more competitive pricing than that of Fannie Mae for the local banks in your region. To do this, you will need to come up with a better alternative to Fannie’s LLPA matrix to make the resulting scheme more attractive to your member banks than selling to Fannie. You may assume for now that your local banks will simply choose the better of the two prices if they decide to sell, and furthermore they will sell to you in the case of a tie. If you want to be more ambitious, you may think up other sources of external data that you can employ to identify specific signals of higher and lower default risk based on your analysis and incorporate them into a new scheme that you will offer to your local banks in purchasing their originating loans.\nYou can use the risk sharing approach as a backup and try to work on a risk refinement approach as your overall goal. For the risk refinement goal, you will first need to spot opportunities for recognizing risk more finely. As a prelude to doing this, you should evaluate the same set of historical loans originating in your assigned region, and develop a predictive model for the potential loss from these loans based on their features. By comparing this against a similar model for a randomly sampled subset of similar mortgages (same vintage, but sampled from all over the nation), you should be able to identify any key differences in the driving variables between the two. This may lead you to spot opportunities for you to adjust your pricing. While it would be simple to just adjust the same LLPA framework in the pricing (that varies according to borrower’s FICO and LTV), you may also add conditions based on on other highly predictive features of these loans in your differential analysis.\nHere is a non-exhaustive list of suggestions for models to enhance your ability to better understand and quantify risk for a segment of your population.\n\nSpot differences in either or both the probability of default and the severity (the percentage or dollar value of the loss given default) for loans with the same risk characteristics (LTV, FICO score, purpose) between the loans for the whole nation and those for the region you are studying; You can use this to try and predict what might be the drivers for them - you could potentially find external data such as home price indices, unemployment rates or other such macroeconomic features that might affect loan payments, or any other local events or developments. The example of marking a loan value to market using the home price indices in the second tutorial webinar-102.pdf could provide some ideas for your analysis. The corresponding online tutorial is here.\nEnrich the modeling of the defaults on each cohort of loans over their lifetime by adding a predictive model for this percentage rather than the summaries you used last week. These loss percentages over the life of the loan can be correlated with other macroeconomic factors (such as the average mortgage rate or average unemployment rate etc. over that year) so you might use a simple regression to predict this. Alternately, you can use a two stage model where for each year of the loan, you can predict a probability of default say using a logistic regression, and then predict the loss given default amount as a function of the current tenure of the loan and all other relevant risk factors, external macroeconomic factors and geographic factors. There is a lot of scope to deploy predictive models to the default calculation such as these examples from a draft paper.\nWhile this is not a predictive exercise, you could redo the analysis of the Fannie pricing for three distinct regimes: loans purchased in 2008, 2010 and in 2013 and examine if you can see any structural differences in the pricing, and how fair Fannie’s pricing was in these regimes. You can find LLPA matrices from these three years in our Case page.\n\nFor your fourth week, you will detail the specific predictive model(s) you decided to build and present the results of your predictive model, an explanation of the key drivers and the point of differentiation of your regional model from that of a national level model, and provide some ideas for exploiting these differences in a new pricing design, or other competitive schemes. Alternately, you might provide a deeper understanding of the evolution of Fannie’s pricing over three regimes. Either way, this week’s work should prepare you for the next step of finding a better prescription for your member banks.\n\n\n\nThe last stage of your modeling is the prescriptive part of designing a more attractive offer for originators than Fannie’s pricing.\nAt a minimum, you could follow a risk sharing approach and find a more competitively priced LLPA matrix assuming reasonable risk sharing parameters. For example, you may assume that all loans originating from the top 3-5 lenders in your region (assume they are local banks for now, even if that is not the case in the data) will be willing to work you as shareholders in your bank to provide an escrow pool containing a small fraction of the value of the most risky loans (e.g., 5% of the value of all loans with FICO scores below 650 and LTV above 70%). Any defaults from such loans originating from these banks will first draw down say up to 20% of the default UPB (unpaid balance) from this escrow. Based on your previous understanding of variations in your geographic region, you can now try to find conditions on the loans, the escrow amounts and the draw down rules under default that would allow you to offer better LLPA prices than Fannie. You can then do a sensitivity anlysis of these parameters and the resulting prices to determine particularly attractive schemes for offer. For example, larger fractions of the UPB covered by the escrow can lead to better price adjustment. The way in which this might be implemented is for your Federal Home Loan bank to establish first liens on collateral worth the designated escrow amount from the participating banks, which can then be drawn down in case of a default obeying the specified conditions.\nIf you are successful in the risk refinement approach, you may design better prices based on specific features that you were able to identify to refine the risk: e.g. if you find home price indices in some of the MSA’s you are studying serve as leading indicators of default behavior and can see a clear pattern in the evolution of this indicator, this could lead you to propose specific better prices over all LTV categories. You could also combine the risk refinement and sharing approaches: e.g. you may think of a scheme where an escrow is used to cover losses due to excessive prepayments for refinancing by enforcing a draw-down condition that is triggered by the Fed’s interest rates breaching very low values. Whatever you decide, you should propose a range of design parameters that you would like to test over, do a sensitivity analysis and and finally provide a set of price adjustments in your offer with appropriate conditions to your local banks.\nFor the report, you could continue to refine any predictive modes you built in the previous week but add at least one element of what specific design parameters (decision variables) you will be changing to offer a better product. If you do not have very good signals for risk refinement, you should at least follow the risk sharing approach and provide an analysis based on an escrow account to cover all or part of the default for some risk pools. The report for this week should contain a description of the design of better pricing you plan to offer and the variables and parameters over which you will continue to investigate to finalize one.\n\n\n\nIn this penultimate week, you should perform a set of analyses based on either of your approaches to arrive at a specific (or narrow) set of recommendations. For the task this week, describe in detail the performance of the one to three main schemes you finally arrived at. To do this, based on historical data, this time from a later period than what you trained your models on, you should provide a sense of the profitability of your schemes or revised pricing to your member banks. The key output here is the increase in profitability for the shareholders by using your new design by doing a counterfactual.\n\n\n\nYour task at this point will switch to building a convincing case for your audience about your understanding, analysis and any particularly interesting and strong predictive signals you identified and how they can be a source of designing the better pricing adjustments.\nThe final presentation should highlight your design, its profitability, and the associated assumptions and risks with achieving these outcomes. You will need to submit a deck of slides a day before the last class to allow your referees to look through them - these referees are quite likely to include a current President and Executive VP of one of the Federal Home Loan banks in the U.S. Since they will be very close to the business, you can expect questions to be of a more business problem driven nature than technical. However, it is your key responsibility to communicate clearly the various technical models that you have employed in arriving at your predictions, as well as any mechanics that these models learn or assume. It would be important to show the power of some of the analytical modeling techniques you have now mastery over to gain better understanding, identify better predictions and put these together in a better decision framework - that is the crux of the technical work in your case. For the final presentation, you can plan for 15 minutes including Q & A which will allow for about 10 slides. Please think carefully about how you communicate your analysis and the top line messages. You slides are due by midnight the day before the last class so I can share them with the other referees.\nYour final report can be up to ten pages long. Include a one-page executive summary that contains the main recommendations, profitability opportunities and insights that drive them. Craft the report in a linear storytelling fashion to build the case for your recommendation, what analyses allowed you to narrow down on the choices you made. Make sure to use at least one figure or table in each regular page of the report that brings alive the models and outputs from your analysis.\n\n\n\n\nA bibliography of writings on Fannie Mae, Freddie Mac and the Federal Home Loan Banks can be found here."
  },
  {
    "objectID": "case_v0.html#background",
    "href": "case_v0.html#background",
    "title": "Expository Notes",
    "section": "",
    "text": "Fannie Mae (FNMA) is a U.S. government sponsored enterprise that was founded in 1938 during the great depression with the purpose of expanding the secondary mortgage market. Along with Freddie Mac (FHLMC) that was created in 1970 to buy mortgages in the secondary market and pool them to sell as mortgage-backed securities, Fannie Mae helps lenders re-invest their assets and hence increase the lenders in the market. In this way, these two organizations reduce the reliance on local associations and create a more liquid market for mortgage-indexed debt.\nThe underlying mechanism for Fannie Mae to buy loans from smaller local banks that originate mortgages is to specify a set of conforming mortgage parameters - these are re- quirements that the loans should meet to provide an acceptable level of risk for purchase by Fannie Mae. They typically include conditions such as the following: the LTV (loan-to- value) ratio of the loan should be small, which signals that the borrower has at least come up with sufficient funds otherwise that will be liable for collection in case of default2.\nThe risk profile of a typical fixed-rate 30-year mortgage consists of two components: credit risk and interest rate risk. The former consists of the default risk where the borrower stops paying back the monthly installments turning the loan into delinquent state and moving it towards collection. The interest rate risk is associated with varying (U.S. Treasury’s) interest rates for investing money in risk-less deposits over the future term of the loan, which in turn is highly correlated with prevailing mortgage rates. An increase in interest rates will make the loans more profitable while a decrease will potentially lead more borrowers to refinance. Thus, this risk includes the additional optionality risk of the borrower refinancing the loan and prematurely terminating the stream of loan payments by prepayment. The business model of Fannie Mae is to purchase loans from originating banks so as to de-risk the credit risk associated with the loan; the purchased loans are then bundled into mortgage-backed securities that do not suffer credit risk but only the residual interest rate risk3.\nOne of the key mechanisms by which Fannie Mae is able to enable a liquid market for loans is to provide a “cash window” before the closing of every mortgage in which it is open to buy these mortgages at pre-determined rates. For mortgages that have been lent based on the conforming mortgage parameters and hence eligible for purchase by Fannie Mae as determined in their eligibility matrix, during the cash window, Fannie is open to purchasing them. When it buys these loans, it pays originators based on the features of the loan (such as the LTV or loan-to-value ratio, and borrower characteristics such as FICO score). The value at which the loans will be purchased are based on a baseline value per $100,000 along with adjustments primarily based on the LTV and borrower’s FICO score, as well as other features of the mortgage such as if the purpose was COR (Cash-out refinance), and whether the borrower obtained Subordinate Financing or Minimum Mortgage Insurance Coverage for the loan.\n\n\nThe adjustments to the baseline price are detailed in a loan-level price adjustment (LLPA) matrix that is published by Fannie Mae and sporadically updated based on market conditions. These price adjustments are cumulative in that a mortgage that has purpose COR and obtains Minimum Mortgage insurance coverage will pay the sum of the LLPA adjustments for these two features in the price. As an example, consider Example 1b from Page 6 of the December 2010 LLPA matrix published with this case. The 30-year fixed rate mortgage for the purpose of Cash-Out Refinancing (COR) by a borrower with credit score 680 on a loan with LTV (loan-to-value) ratio of 85% and obtaining Minimum Mortgage Insurance (MI) and originating in May 2011 has the following adjustments: 0.25% for the basic Adverse Market Delivery Charge (AMDC) of taking up the mortgage4, the credit score and LTV combination of the borrower adds 1.5% (from Table 2 bottom half for loans originating after April 1, 2011) the COR purpose for a loan with this LTV-FICO combination adds 2.5% (Table 3 bottom half) and finally the MI adds 0.125% (Table 5) for a total of 4.375% to the price of the mortgage.\nMortgage originators (like banks and brokers) use the LLPA matrix in the following way. First they obtain a pricing matrix that is published by Fannie Mae based on the prevailing mortgage rate for 30-year fixed rate mortgages (See the example june2019pricing.pdf that prevailed around June 15th 2019 provided with this case). This matrix details the discounts or premiums that Fannie will pay in terms of the note rate (the annual interest rate at which the loan is closed) and the time window within which the closing will occur (thus, varying durations for the cash window). Intuitively, notes with higher rates and closed closer to the current day will fetch higher premiums. It is important to note that these prices are in basis points which are simply the percentage of the prices. Thus, the pricing premium of 5.6845 points for a loan with note rate of 5.875% to be closed within 5 days means that a loan for 100,000 USD with these parameters will fetch the originator of the mortgage 105,685 USD by selling to Fannie. Typically the note rate at which the premium or discount is at zero roughly corresponds that day’s 30-year fixed rate announced by Fannie and available as a historical record here.\nConsider an example of a loan originator who is trying to determine the best rate to offer a borrower by studying the prevailing pricing matrix, the LLPA matrix and her own costs and margins. Suppose a loan for 100,000 USD is to be closed on 18 June 2019, and the pricing when the loan is to be locked occurs on June 15th when the Fannie rate is between 3.375% and 3.5% and hence reflected in the enclosed june2019pricing.pdf document. Also, suppose the sum of the costs incurred by the originator plus the margin she would like to earn on closing the loan total 2,000 USD and hence 2 points. Furthermore, suppose the loan is taken out for the purpose of Cash-Out Refinancing (COR) by a borrower with credit score 690 on a loan with LTV (loan-to-value) ratio of 80%: the LTV-credit score combination in the LLPA matrix adds 1.75 points and the COR purpose adds another 1.75 points for a total of 3.5 points. Thus the originator needs to make 2 + 3:5 = 5:5 points above par to sell the loan to Fannie. This involves looking up the june2019pricing.pdf table again for the note rate that is closest to fetching 5.5 points above par (i.e., 105,500 USD), which in this case corresponds to a 5.875% final interest rate for the customer. When Fannie buys such a loan, it pays the originator 5.6845 (from the pricing matrix) minus the 3.5 points LLPA adjustment and hence a total of 102,185 USD, which in turn will cover the 2 points expectation for the costs and margins of the originator.\n\n\n\nTo reconcile if Fannie’s purchases made better or worse than what they were purchased for, you should ideally simulate for each loan purchased by Fannie, the price it actually paid using the pricing matrix based on the note rate and closing window, and examine the stream of cash flows it received regarding this loan (including default costs if any). However, these prices Fannie paid are not available publicly. Moreover, each of the originators may have different costs and also seek different margins which would further obscure the calculation, since we will be unable to impute this addition that they use in computing the final interest rate in the originating process.\nTo avoid the above two data difficulties, you can simplify the problem and just examine whether the LLPA price adjustments of Fannie are correctly benchmarked against the baseline set of loans on which these LLPA adjustments are zero. To do this, you can look at the additional default losses suffered by Fannie in each of the LLPA adjustment buckets other than a set of baseline groups (with zero LLPA adjustment) and examine how the net losses from one of these groups compares with the aggregate adjustment fee collected by Fannie on that group."
  },
  {
    "objectID": "case_v0.html#task-outline",
    "href": "case_v0.html#task-outline",
    "title": "Expository Notes",
    "section": "",
    "text": "The network of Federal Home Loan banks were established as a cooperative of local banks and lending institutions within a region to further increase liquidity in the mortgage markets. For the purpose of this case, assume that you are an Analytics Director of the Federal Home Loan bank of your region (which will be assigned based on the group that you are part of). Your goal is to design new schemes to improve the profitability of your bank without negatively impacting the mandate of increasing and enabling the liquidity of the markets for mortgages. This typically involves offering more attractive pricing schemes than Fannie in buying back loans from your local originators.\n\n\nYou do not have any report to turn in the first week. You can take the time to read the case and understand the business context as you read the case.\n\n\n\nFannie Mae has made a portion of the data about the 30-year fixed rate loans it has purchased since 2000 and their performance over time available publicly here. To access this, you will first need to register at this website with a valid email address and basic information.\nTo familiarize yourself with the data there are two tutorials in this main information website, along with an overview as well as sample code in R and SAS to import and combine the two data tables as explained in the tutorial. Your first task for the first week will be to work through the first tutorial and build a flat combined data file5. You can then refine it for the specific subsamples of interest for your team (city, metropolitan statistical areas/MSA or state) later by filtering the full data using a subset of three-digit ZIP codes and/or MSA’s. You second task will be to build informative sample statistics from the data set to familiarize yourself with its characteristics.\nWe will examine all the loans purchased in 2010 in or around four metropolitan areas: Atlanta, Dallas, New York and San Francisco. Based on the team you are in, you will work with one of these areas in more detail. In choosing which subset of loans you want to analyze as a team in your region, you may want to assemble three levels of data sets: First, you can assemble a set for a couple of three-digit ZIP codes that define a specific city in your region (e.g., Atlanta - 303 and 311, Dallas - 752 and 753, New York - 100, 101 and 102, San Francisco - 941). At the next level you can find the appropriate MSA’s that encompass a slightly larger area. Finally, you can simply gather the data set for the whole state using three-digit ZIP code refinements from the information available here.\nFor Week 2, as a group you can first follow the steps in the tutorial for the whole data set of acquisitions from 2010. Note that in addition to downloading the acquisition data files for the four quarters of 2010, you will need to download the performance data for all quarters for all years from and including 2010. Plan to allocate enough time to download the data - if you have enough space in your machine, it may be more convenient to download all the files since 2000 as one big download (nearly 25 GB).\nAs your submission for the second week, I would like you to carry out an analysis similar to what is given in the webinar-101.pdf in the Fannie Mae website. Note that they also detail the data cleaning choices they made and how the performance files over time have been combined into a single data file with one row per loan. You will find that watching the accompanying online tutorial will clarify a lot of the computations. Repeat this computation in your favorite computing environment for all loans acquired in 2010, and for the subsets in your regions (city, MSA, state) to spot any variations.\nSince we will eventually be concerned with whether the price adjustments are accurate and try to identify opportunities from any deviations, there are two measures you can pay particular attention to in your analysis. The first is the loss suffered from a set of mortgages as a percentage of the total original principal - the reason why this will be important is because the difference in this measure between a baseline group of loans (with zero LLPA adjustment) and a group with a specific adjustment will give us an estimate of the additional default cost to Fannie of serving the second group against the baseline. This can then directly be compared with the price adjustment in the LLPA matrix which is also in percentages. The Net Loss Rate calculation in the first tutorial will give you a good starting point to do this. The other measure you will eventually need to grapple with is future defaults from currently active loans. For this, it may be useful to find simple summaries and visualizations of the default behavior of set of loans that have been active for 5 or 10 years, and how they evolve over time. Other than these two speciic concerns, you can and should be open to finding interesting summaries and descriptive patterns in the data at this early stage to deepen your understanding of the data in the case context.\n\n\n\nYou should begin assembling the datasets for the loans in your regions this week if you did not already in the previous. Your task this week is to determine how the loans serviced by Fannie Mae performed in their portfolio. In other words, your goal is to assess the quality of Fannie’s LLPA adjustments.\nFor this, we will work with the set of loans in your region purchased by Fannie during 2010. As an example, let us consider a specific risk cohort which is the subset of loans with primary borrower FICO score range 660-679 and LTV range 75.01-80% having a LLPA adjustment of 2.5%, which is one of the primary risk buckets in the December 2010 LLPA matrix with positive LLPA. Note that all loans have a AMDC of 0.25% which when added to the negative LLPA adjustments in the first column of Table 2 gives a 0% adjustment. Thus we can use as a baseline group the set of loans with primary borrower FICO scores 700 or above and LTV up to 60%.\nFor each of these different risk cohorts,, you will need to determine the default losses as a percentage of the original principal. This is also termed the Net Loss Rate in the tutorials. The calculation is relatively direct: Using the combined data set from the first week, filter out the group of loans of interest (say baseline or a specific risk cohort). For all the loans taken out in 2010 in the group, compute the net loss for the group as a percentage of the total principal.\nNote that for the loans that are still active from the portfolio, you still do not know their future performance. For this week, you may try to build a simple summary average of the percent of the original loan value of a cohort that defaults in each of the first through later years. Next week, you can try to refine this by trying to build a model for predicting the percentage of default in each of the years in the lifetime of the loan. For the simple summary, you will need to collect loans of your risk cohort over as many years as you can in the past, and build a table of the fraction of the original principal that defaulted each of the years that the loan has been active. For those originating in 2000, you have nearly 18 years of such data whereas for the cohort beginning in 2015, you will only have this for the first three years. Overlaying these summaries of default percentage across the life of the loan and averaging, you can get a measure of the default loss as a percentage of the principal in each year for each cohort6. You can then use these summaries to extrapolate the losses on the loans from 2010 for the next 20 years (You will still need to make some assumptions about the losses in the last ten years of the loan for which you will have no data). Whatever you do, please be very clear in your weekly update about the assumptions you made in carrying out this audit calculation.\nRedo the loss differential calculation across different risk cohorts based on the LTV and FICO ranges, as well as other potential risk factors like Cash Out Refinancing as loan purpose and Investment Property, which also have corresponding LLPA adjustments. Do this across the whole data set as well as the different levels in your regions to find any significant deviations. At the end of this week, you will prepare a report summarizing your analysis of the profit/loss accrued to Fannie as a result of its pricing, and any opportunity this presents to your own Federal Home Loan bank to offer intermediation based on any geographic variation you identified.\n\n\n\nThe next step of the analysis is a prelude to help you design new products to offer your member banks. At a minimum, you may attempt to buy loans from your local originators (assume all of them to be member banks of your organization) by providing them a more competitive price for the loans they originate than is reflected in Fannie’s pricing. However, to be able to do this effectively, you will either need more information for risk arbitrage than what is available to Fannie, or you will need a way to cover default losses more reliably than Fannie is able to. In other words, you should either be able to get a better estimate than Fannie of the credit worthiness of the borrowers in your region whose loans you propose to buy, or you should have the potential for a risk-sharing agreement with your local banks to mitigate any risks that might arise from your better pricing. Both are plausible avenues to investigate, and let us call them loosely “risk refinement” and “risk sharing” respectively.\nFor the former (risk refinement), you may be able to find and use additional data about specific metropolitan areas that might aid in your assessment of property value evolution which in turn may make some loans more or less advantageous due to the resulting changes in their LTV. For the latter (risk sharing), since the local banks are the actual shareholders and hence owners of your bank, they may be amenable to an agreement where your more competitive pricing is accompanied by a credit buffer that you set up with them that will be a first pool of resource to turn to in the eventuality of loss events from that bank’s portfolio. This buffer can be adjusted in size and depletion rules based on the performance of the originated loans from that bank.\nFor the case, I would like you to propose a more competitive pricing than that of Fannie Mae for the local banks in your region. To do this, you will need to come up with a better alternative to Fannie’s LLPA matrix to make the resulting scheme more attractive to your member banks than selling to Fannie. You may assume for now that your local banks will simply choose the better of the two prices if they decide to sell, and furthermore they will sell to you in the case of a tie. If you want to be more ambitious, you may think up other sources of external data that you can employ to identify specific signals of higher and lower default risk based on your analysis and incorporate them into a new scheme that you will offer to your local banks in purchasing their originating loans.\nYou can use the risk sharing approach as a backup and try to work on a risk refinement approach as your overall goal. For the risk refinement goal, you will first need to spot opportunities for recognizing risk more finely. As a prelude to doing this, you should evaluate the same set of historical loans originating in your assigned region, and develop a predictive model for the potential loss from these loans based on their features. By comparing this against a similar model for a randomly sampled subset of similar mortgages (same vintage, but sampled from all over the nation), you should be able to identify any key differences in the driving variables between the two. This may lead you to spot opportunities for you to adjust your pricing. While it would be simple to just adjust the same LLPA framework in the pricing (that varies according to borrower’s FICO and LTV), you may also add conditions based on on other highly predictive features of these loans in your differential analysis.\nHere is a non-exhaustive list of suggestions for models to enhance your ability to better understand and quantify risk for a segment of your population.\n\nSpot differences in either or both the probability of default and the severity (the percentage or dollar value of the loss given default) for loans with the same risk characteristics (LTV, FICO score, purpose) between the loans for the whole nation and those for the region you are studying; You can use this to try and predict what might be the drivers for them - you could potentially find external data such as home price indices, unemployment rates or other such macroeconomic features that might affect loan payments, or any other local events or developments. The example of marking a loan value to market using the home price indices in the second tutorial webinar-102.pdf could provide some ideas for your analysis. The corresponding online tutorial is here.\nEnrich the modeling of the defaults on each cohort of loans over their lifetime by adding a predictive model for this percentage rather than the summaries you used last week. These loss percentages over the life of the loan can be correlated with other macroeconomic factors (such as the average mortgage rate or average unemployment rate etc. over that year) so you might use a simple regression to predict this. Alternately, you can use a two stage model where for each year of the loan, you can predict a probability of default say using a logistic regression, and then predict the loss given default amount as a function of the current tenure of the loan and all other relevant risk factors, external macroeconomic factors and geographic factors. There is a lot of scope to deploy predictive models to the default calculation such as these examples from a draft paper.\nWhile this is not a predictive exercise, you could redo the analysis of the Fannie pricing for three distinct regimes: loans purchased in 2008, 2010 and in 2013 and examine if you can see any structural differences in the pricing, and how fair Fannie’s pricing was in these regimes. You can find LLPA matrices from these three years in our Case page.\n\nFor your fourth week, you will detail the specific predictive model(s) you decided to build and present the results of your predictive model, an explanation of the key drivers and the point of differentiation of your regional model from that of a national level model, and provide some ideas for exploiting these differences in a new pricing design, or other competitive schemes. Alternately, you might provide a deeper understanding of the evolution of Fannie’s pricing over three regimes. Either way, this week’s work should prepare you for the next step of finding a better prescription for your member banks.\n\n\n\nThe last stage of your modeling is the prescriptive part of designing a more attractive offer for originators than Fannie’s pricing.\nAt a minimum, you could follow a risk sharing approach and find a more competitively priced LLPA matrix assuming reasonable risk sharing parameters. For example, you may assume that all loans originating from the top 3-5 lenders in your region (assume they are local banks for now, even if that is not the case in the data) will be willing to work you as shareholders in your bank to provide an escrow pool containing a small fraction of the value of the most risky loans (e.g., 5% of the value of all loans with FICO scores below 650 and LTV above 70%). Any defaults from such loans originating from these banks will first draw down say up to 20% of the default UPB (unpaid balance) from this escrow. Based on your previous understanding of variations in your geographic region, you can now try to find conditions on the loans, the escrow amounts and the draw down rules under default that would allow you to offer better LLPA prices than Fannie. You can then do a sensitivity anlysis of these parameters and the resulting prices to determine particularly attractive schemes for offer. For example, larger fractions of the UPB covered by the escrow can lead to better price adjustment. The way in which this might be implemented is for your Federal Home Loan bank to establish first liens on collateral worth the designated escrow amount from the participating banks, which can then be drawn down in case of a default obeying the specified conditions.\nIf you are successful in the risk refinement approach, you may design better prices based on specific features that you were able to identify to refine the risk: e.g. if you find home price indices in some of the MSA’s you are studying serve as leading indicators of default behavior and can see a clear pattern in the evolution of this indicator, this could lead you to propose specific better prices over all LTV categories. You could also combine the risk refinement and sharing approaches: e.g. you may think of a scheme where an escrow is used to cover losses due to excessive prepayments for refinancing by enforcing a draw-down condition that is triggered by the Fed’s interest rates breaching very low values. Whatever you decide, you should propose a range of design parameters that you would like to test over, do a sensitivity analysis and and finally provide a set of price adjustments in your offer with appropriate conditions to your local banks.\nFor the report, you could continue to refine any predictive modes you built in the previous week but add at least one element of what specific design parameters (decision variables) you will be changing to offer a better product. If you do not have very good signals for risk refinement, you should at least follow the risk sharing approach and provide an analysis based on an escrow account to cover all or part of the default for some risk pools. The report for this week should contain a description of the design of better pricing you plan to offer and the variables and parameters over which you will continue to investigate to finalize one.\n\n\n\nIn this penultimate week, you should perform a set of analyses based on either of your approaches to arrive at a specific (or narrow) set of recommendations. For the task this week, describe in detail the performance of the one to three main schemes you finally arrived at. To do this, based on historical data, this time from a later period than what you trained your models on, you should provide a sense of the profitability of your schemes or revised pricing to your member banks. The key output here is the increase in profitability for the shareholders by using your new design by doing a counterfactual.\n\n\n\nYour task at this point will switch to building a convincing case for your audience about your understanding, analysis and any particularly interesting and strong predictive signals you identified and how they can be a source of designing the better pricing adjustments.\nThe final presentation should highlight your design, its profitability, and the associated assumptions and risks with achieving these outcomes. You will need to submit a deck of slides a day before the last class to allow your referees to look through them - these referees are quite likely to include a current President and Executive VP of one of the Federal Home Loan banks in the U.S. Since they will be very close to the business, you can expect questions to be of a more business problem driven nature than technical. However, it is your key responsibility to communicate clearly the various technical models that you have employed in arriving at your predictions, as well as any mechanics that these models learn or assume. It would be important to show the power of some of the analytical modeling techniques you have now mastery over to gain better understanding, identify better predictions and put these together in a better decision framework - that is the crux of the technical work in your case. For the final presentation, you can plan for 15 minutes including Q & A which will allow for about 10 slides. Please think carefully about how you communicate your analysis and the top line messages. You slides are due by midnight the day before the last class so I can share them with the other referees.\nYour final report can be up to ten pages long. Include a one-page executive summary that contains the main recommendations, profitability opportunities and insights that drive them. Craft the report in a linear storytelling fashion to build the case for your recommendation, what analyses allowed you to narrow down on the choices you made. Make sure to use at least one figure or table in each regular page of the report that brings alive the models and outputs from your analysis."
  },
  {
    "objectID": "case_v0.html#references",
    "href": "case_v0.html#references",
    "title": "Expository Notes",
    "section": "",
    "text": "A bibliography of writings on Fannie Mae, Freddie Mac and the Federal Home Loan Banks can be found here."
  },
  {
    "objectID": "case_v0.html#footnotes",
    "href": "case_v0.html#footnotes",
    "title": "Expository Notes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis case was developed by Professor R. Ravi in 2019 for the course 46-889 titled Business Value through Integrative Analytics, in the Master of Science in Business Analytics program at the Tepper School of Business, Carnegie Mellon University. Ravi is grateful to Charles Burke and Lee Cammerer at the Federal Home Loan Bank of Dallas for their insights and Kalyan Madhavan at the Federal Home Loan Bank of Dallas for his advice in developing this case.↩︎\nThese initial investments are not unlike deductibles paid in insurance policies to avoid the moral hazard problem; If the borrower lacks sufficient funds to fulffill this requirement, she may take out mortgage insurance for the remaining value that will pay out the some part of the value of the property in case of default.↩︎\nThe MBS’s are then further sold via auction to real-estate arms of large financial firms via a BWIC (“bee-wick”) process (bids wanted in competition) that specifies the characteristics of the bundle for sale along with a deadline for bidding.↩︎\nThe ADMC originated in the financial crisis of 2008 and was eliminated and incorporated into the basic rates around 2015.↩︎\nThe R and SAS files provided with the tutorials are slightly outdated in that they process data that was published around 2015. The data has been enhanced since to add three additional fields to both the acquisition and performance files, so you will need to modify them slightly to read in the enriched data.↩︎\nDo not worry about the time discounting of these losses for now. This time value adjustment will nearly vanish if the prices charged by Fannie are themselves invested at the risk-free rate or better which is the case.↩︎"
  },
  {
    "objectID": "case_G.html",
    "href": "case_G.html",
    "title": "Case G - Communicating Analytics",
    "section": "",
    "text": "Learning objectives\nCommunicate complex analytical ideas with clean messages and graphics.\n\n\nMain business question\nPick one of the previous 5 modules (exclude initial business/data undersatnding) and present it in an effective way.\n\n\nCase detailed questions\n\nWhat is the main message/insight from your presentation?\nWhat do you expect the presentation will lead the audience to do?\nHave you flagged potential shortcomings and models assumptions that may derail your recommendations?\n\n\n\nMapping to course\nLast week on communicating analytics.\n\n\nOther notes\n\nUse rubrics and learnings from communications courses to evaluate\nEmphasize delivery, careful use of text, graphics, color, chart"
  },
  {
    "objectID": "case_E.html",
    "href": "case_E.html",
    "title": "Case E - Prescription: Pricing default risk",
    "section": "",
    "text": "Learning objectives\nUse optimization models to determine at what price a risky loan may be purchased at origination in return for its uncertain stream of cash repayments.\n\n\nMain business question\nGiven all the loan features at origination, how much would you accept as sufficient reward for taking over the loan?\n\n\nCase detailed questions\n\nWhat model would you use to determine a price for the default risk of a loan?\nDo prepayments help you or hurt you in assuming this risk?\nWhat key features of the loan at origination will you identify to price the loan differentially? How will you deteremine these features?\n\n\n\nMapping to course\nFifth exercise on simple optimization models.\n\n\nOther notes\n\nThis is really a more detailed version of the previous two exercises on predicting default and the losses from it. The top features detereming these two (such as LTV ratio and FICO score of borrower) will determine the risk of buying the loan.\nBest to guide students to deteremine these factors and how to differentially increase the price as the values of these factors worsen."
  },
  {
    "objectID": "case_C.html",
    "href": "case_C.html",
    "title": "Case C - Classification: Predict loan default at origination",
    "section": "",
    "text": "Learning objectives\nApply classification model to predict loan default; Define default carefully.\n\n\nMain business question\nCan the default risk of a loan application be learned from the loan application features?\n\n\nCase detailed questions\n\nDefine carefully when a loan defaults (e.g. reaches default status within the first N years of its origination)\nRun a simple logistic regression model for predicting if a loan will default\nRun tree models for the same task\nCompare the models for their benefits and shortcomings\n\n\n\nMapping to course\nThird exercise on classification models\n\n\nOther notes\n\nDefine default as defaulting in the first 3 years, or as defaulting in the next 3 years (moving ahead to year 5 of the loan). Show that the prediction models are different - why is this the case? Explain automatic censoring of data with prepaid and defaulted loans in the cohort. Motivate survival models.\nDefine prepayment and use the same methods to predict prepayment ‘risk’. How can this be useful for mortgage financing? What will it depend upon? (Interest rates)"
  },
  {
    "objectID": "case_A0.html",
    "href": "case_A0.html",
    "title": "Case A0 - Data Understanding: Manipulating Modern Business Scale Datasets",
    "section": "",
    "text": "Learning objective\nUnderstand the complexity and integrity of large data systems and learn how to work with them.\n\n\nMain questions\nIdentify source of relevant data for the problem, procure it and clean it in preparation for future analysis. While an ‘engineering’ task, it is becoming necessary as part of a skill set for business analysts.\n\n\nDetailed questions\n\nDownload Data\nAdapt given scripts to consolidate multiple records per loan into one\nDevise methods to tackle missing or incomplete data\n\n\n\nMapping to course\nOptional data preparation module when time permits.\n\n\nOther notes\n\nNeed links to technical notes on data extraction with low memory usage and other issues related to memory and computing."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Authors\nR. Ravi and Amr Farahat\n\n\nLicense\nThis case is licensed under Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International license\n\n\nVersion\nVersion: 0.0.2\nDate: 2023-10-16\n\n\nHistory\nThis case was first developed by Professor R. Ravi in 2019 for the course 46-889 titled Business Value through Integrative Analytics, in the Master of Science in Business Analytics program at the Tepper School of Business, Carnegie Mellon University.\n\n\nAcknowledgements\nRavi is grateful to Charles Burke and Lee Cammerer at the Federal Home Loan Bank of Dallas for their insights and Kalyan Madhavan at the Federal Home Loan Bank of Dallas for his advice in developing this case."
  },
  {
    "objectID": "case_A.html",
    "href": "case_A.html",
    "title": "Case A - Business Understanding: Exploratory Data Analysis",
    "section": "",
    "text": "Learning objectives\nDefine the business problem using specific metrics and targets, paying attention to all stakeholders.\n\n\nMain business question\nWhat key insights about 30-year mortgage loan repayments can you glean from the historical record? How can you summarize the data best?\n\n\nCase detailed questions\n\nOut of all the features available, what are the top 10/20/30 that you would retain for further analysis of loan repayment patterns?\nWhat are your top three hypotheses about factors predicting “ZBC”?\nWhat explains loan volume variations across years (e.g. 2000-2005)?\nCreate summary tables and verify their validity against those published by FNMA\n\n\n\nMapping to course\nFirst exercise on data and business understanding.\n\n\nOther notes\n\nAugment with geographic variation (assign different regions to different teams)"
  },
  {
    "objectID": "case_B.html",
    "href": "case_B.html",
    "title": "Case B - Descriptive Analysis",
    "section": "",
    "text": "Learning objectives\nIdentify useful patterns among the features and the data points and relate it to business concerns.\n\n\nMain business question\nHow can you find systematic patterns in the data?\n\n\nCase detailed questions\n\nPerform an unsupervised analysis of loans using K-Means Clustering and see if your clusters correspond to risk classes\nPerform PCA to find correlated clusters of features\n\n\n\nMapping to course\nSecond exercise on descriptive analysis.\n\n\nOther notes\n\nName and explain KMeans clusters using parallel line plots\nExplain PC components using loadings and extremal points\nVary the number of clusters in KMeans and arrive at an ‘optimal’ number\nMaybe use sparse PCA"
  },
  {
    "objectID": "case_D.html",
    "href": "case_D.html",
    "title": "Case D - Regression: Predict Net Returns on a Loan or Loss Given Default at origination",
    "section": "",
    "text": "Learning objectives\nUse regression models to predict net profit/loss from loans.\n\n\nMain business question\nCan we directly determine the profitability of taking on a loan in dollar terms at origination time?\n\n\nCase detailed questions\n\nDefine the LGD (loss given default): for loans that defaulted, the fraction of the total principle that was unpaid.\nRun regression models with feature selection (say using L1, L2, Elastic Net, Regression Trees, Random Forests…) to predict loss given default at loan origination\nRe-run similar models to predict net returns from a loan using features at loan origination\nWhy might such models not be very robust?\n\n\n\nMapping to course\nFourth exercise on regression models\n\n\nOther notes\n\nAdd prepayment into the net return calculation of the loan (which also reduces the net return like losses do but less severely)"
  },
  {
    "objectID": "case_F.html",
    "href": "case_F.html",
    "title": "Case F - Reconcile buying loans at scale",
    "section": "",
    "text": "Learning objectives\nExtend insight from previous module to use law of large numbers to buy many loans at scale to derisk them of defaults.\n\n\nMain business question\nFNMA purchases loans from originating banks at different prices based on the loan characteristics using a LLPA matrix. is this pricing accurate?\n\n\nCase detailed questions\n\nAudit the LLPA matrix for a particular cohort of loans (say originating around 2010) for their relative risk profiles.\nVerify if the realized risk is in line with the pricing\nSuggest changes to pricing schemes. Identify opportunities for improving the pricing using special rules based on your own knowledge (such as geographic variation of house prices, employment index and other socio-economic conditions)\n\n\n\nMapping to course\nFinal exercise on oputting together all models about loan repayment into a comprehensive understanding with a succinct price\n\n\nOther notes\n\nEmphasize on finding opportunities to find frictions that local banks can potentially remove"
  },
  {
    "objectID": "case_structure.html#i.-expository-notes",
    "href": "case_structure.html#i.-expository-notes",
    "title": "Case Outline",
    "section": "I. Expository Notes",
    "text": "I. Expository Notes\n\nNote 1: The Home Mortgage Market Supply Chain in the U.S.\n\n\nNote 2: Recognizing and Overcoming the Challenges of Manipulating Large Datasets\n\n\nNote 3: The Pricing and Execution of Whole Loans by Fannie Mae\n\n\nNote 4: Quantifying the Porfitability of Whole Loans"
  },
  {
    "objectID": "case_structure.html#ii.-case-modules",
    "href": "case_structure.html#ii.-case-modules",
    "title": "Case Outline",
    "section": "II. Case Modules",
    "text": "II. Case Modules\n\nCase A: Manipulating Modern Business Scale Datasets\n\nLearnig objectives\nMain business question\nCase detailed questions\nMapping to Ravi’s coure week: 1\nOther notes\n\n\n\nCase B: Exploratory Data Analysis\n\nLearnig objectives\nMain business question\nCase detailed questions\n\nWhat is your hypothesis about factors predicting “ZBC”\nWhat explains loan volume variations across years (e.g. 2000-2005)\n\nMapping to Ravi’s coure week: 2\nOther notes: Divide into basic; geo; unsupervised\n\n\n\nCase C: Estimate Loan Profit/Loss to-Date\n\nLearnig objectives\nMain business question\nCase detailed questions\nMapping to Ravi’s coure week: 2\nOther notes\n\n\n\nCase D1: Classifiy Loan Outcome at Origination\n\nLearnig objectives\nMain business question\nCase detailed questions\nMapping to Ravi’s coure week: 3 (Classification Week)\nOther notes\n\n\n\nCase D2: Classifiy Loan Outcome at Interim Dates\n\nLearnig objectives\nMain business question\nCase detailed questions\nMapping to Ravi’s coure week: 3 (Classification Week)\nOther notes\n\n\n\nCase E1: Predict Loan Profit/Loss at Origination\n\nLearnig objectives\nMain business question\nCase detailed questions\nMapping to Ravi’s coure week: 4 (Regression Week)\nOther notes\n\n\n\nCase E2: Predict Loan Profit/Loss at Interim Dates\n\nLearnig objectives\nMain business question\nCase detailed questions\nMapping to Ravi’s coure week: 4 (Regression Week)\nOther notes\n\n\n\nCase F: Assess Validity of LLPA Matrix\n\nLearnig objectives\nMain business question\nCase detailed questions\nMapping to Ravi’s coure week: 5 (Prescriptive Week)\nOther notes\n\n\n\nCase G: Design Your LLPA Matrix\n\nLearnig objectives\nMain business question\nCase detailed questions\nMapping to Ravi’s coure week: 5 (Prescriptive Week)\nOther notes\n\n\n\nCase H: Examine Georgaphical Variations\n\nLearnig objectives\nMain business question\nCase detailed questions\nMapping to Ravi’s coure week\nOther notes\n\n\n\nCase I: Prescriptive Analytics\n\nLearnig objectives\nMain business question\nCase detailed questions\nMapping to Ravi’s coure week\nOther notes"
  },
  {
    "objectID": "case_structure.html#iii.-resources",
    "href": "case_structure.html#iii.-resources",
    "title": "Case Outline",
    "section": "III. Resources",
    "text": "III. Resources\n\nResource Collection 1: FNMA Public Resources\n\n\nResource Collection 2: Python Resources"
  },
  {
    "objectID": "case_structure.html#iv.-synthetic-datasets",
    "href": "case_structure.html#iv.-synthetic-datasets",
    "title": "Case Outline",
    "section": "IV. Synthetic Datasets",
    "text": "IV. Synthetic Datasets"
  },
  {
    "objectID": "case_structure.html#v.-instructor-resources",
    "href": "case_structure.html#v.-instructor-resources",
    "title": "Case Outline",
    "section": "V. Instructor Resources",
    "text": "V. Instructor Resources"
  },
  {
    "objectID": "data_all_years_sample.html",
    "href": "data_all_years_sample.html",
    "title": "A 10% Sample Covering All Years",
    "section": "",
    "text": "We provide in this section a consolidated dataset comprising a 10% random sample of the 30-year fixed-rate mortgages appearing in Fannie Mae’s July 27, 2023 release of its Single-Family Loan Performance primary dataset. For each year, from 2000 to 2022 inclusive1, we consolidate the data into one row per loan as described in the Data Consolidation section, filter out loans whose original loan term did not equal 360 months, and draw a 10% simple random sample of the remaining loans in that year. Sampled loans from all acquisition years are then combined into a single dataset. The dataset consists of 3,897,758 loans. We provide this dataset in several file formats."
  },
  {
    "objectID": "data_all_years_sample.html#footnotes",
    "href": "data_all_years_sample.html#footnotes",
    "title": "A 10% Sample Covering All Years",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nwe ignore the first quarter of 2023↩︎"
  },
  {
    "objectID": "data_one_year.html",
    "href": "data_one_year.html",
    "title": "A One Year Sample",
    "section": "",
    "text": "Overview\nIn this section, we provide a consolidated dataset pertaining to the Year 2018. The data is derived from Fannie Mae’s July 27, 2023 release of its Single-Family Loan Performance primary dataset. We provide several different versions / formats of this dataset. The first version consists of all loans appearing in Fannie Mae’s dataset that were acquired during 2018. The second version consists of all loans that originated in 2018 (including those acquired later than 2018). The version consists of only 30-year mortgages that orginated in 2018. The datasets are consolidated into one row per loan as desctibed in the Data Consolidation section. Each dataset is provided in a compressed / uncompressed delimited/feather file formats.\n\n\nLoans acquired in 2018\nThis dataset consists of 1,834,469 loans.\n\nDownload delimited &lt;.csv&gt; flat file (delimiter: “|”)\n\nUncompressed (~525MB) filename: &lt;2018_acquired_consolidated.csv&gt;\nCompressed (~105MB) filename: &lt;2018_acquired_consolidated_csv.zip&gt;\n\n\n\nDownload feather &lt;.feather&gt; file\n\nUncompressed (~205MB) filename: &lt;2018_acquired_consolidated.feather&gt;\nCompressed (~115MB) filename: &lt;2018_acquired_consolidated_feather.zip&gt;\n\n\n\n\nLoans originated in 2018\nThis dataset consists of 1,788,020 loans.\n\nDownload delimited &lt;.csv&gt; flat file (delimiter: “|”)\n\nUncompressed (~560MB) filename: &lt;2018_originated_consolidated.csv&gt;\nCompressed (~115MB) filename: &lt;2018_originated_consolidated_csv.zip&gt;\n\n\n\nDownload feather &lt;.feather&gt; file\n\nUncompressed (~270MB) filename: &lt;2018_originated_consolidated.feather&gt;\nCompressed (~135MB) filename: &lt;2018_originated_consolidated_feather.zip&gt;\n\n\n\n\n30-year loans originated in 2018\nThis dataset consists of 1,514,613 loans.\n\nDownload delimited &lt;.csv&gt; flat file (delimiter: “|”)\n\nUncompressed (~485MB) filename: &lt;2018_originated_30year_consolidated.csv&gt;\nCompressed (~100MB) filename: &lt;2018_originated_30year_consolidated_csv.zip&gt;\n\n\n\nDownload feather &lt;.feather&gt; file\n\nUncompressed (~235MB) filename: &lt;2018_originated_30year_consolidated.feather&gt;\nCompressed (~115MB) filename: &lt;2018_originated_30year_consolidated_feather.zip&gt;"
  },
  {
    "objectID": "data_raw.html",
    "href": "data_raw.html",
    "title": "Fannie Mae’s Data",
    "section": "",
    "text": "Fannie Mae publishes granular data representative of a portion of the single-family home mortgages it acquired since year 2000. The data is available on their website here: Fannie Mae Single-Family Loan Performance Data. The types of mortgages included in the dataset, and various exclusions and limitations, are described in items 1 and 2 of this FAQ document.\nThroughout this case, we refer to the July 27, 2023 release1 of the primary2 dataset. This dataset covers approximately 55 million mortgage loans. Monthly performance data is provided for each loan starting from the time of acquisition through to the end of the first quarter of year 2023 or until the loan reaches zero balance (through prepayment, liquidation, etc.) whichever is earlier. Therefore, each record (row) of this dataset is a snapshot of a particular loan at a particular month. The full dataset consists of approximately 2.5 billion records (rows) and approximately 65-70 applicable attributes (columns)."
  },
  {
    "objectID": "data_raw.html#footnotes",
    "href": "data_raw.html#footnotes",
    "title": "Fannie Mae’s Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFannie Mae updates the dataset quarterly↩︎\nWe ignore the accompanying HARP dataset↩︎\nThe “|” delimiter is preferable when some values include commas↩︎\nFannie Mae provides them for compatibility with other dataset disclosures↩︎"
  },
  {
    "objectID": "draft_outline.html#i.-expository-notes",
    "href": "draft_outline.html#i.-expository-notes",
    "title": "Case Outline",
    "section": "I. Expository Notes",
    "text": "I. Expository Notes\n\nNote 1: The Home Mortgage Market Supply Chain in the U.S.\n\n\nNote 2: Recognizing and Overcoming the Challenges of Manipulating Large Datasets\n\n\nNote 3: The Pricing and Execution of Whole Loans by Fannie Mae\n\n\nNote 4: Quantifying the Porfitability of Whole Loans"
  },
  {
    "objectID": "draft_outline.html#ii.-case-modules",
    "href": "draft_outline.html#ii.-case-modules",
    "title": "Case Outline",
    "section": "II. Case Modules",
    "text": "II. Case Modules\n\nCase A: Manipulating Modern Business Scale Datasets\n\nLearnig objectives\nMain business question\nCase detailed questions\nMapping to Ravi’s coure week: 1\nOther notes\n\n\n\nCase B: Exploratory Data Analysis\n\nLearnig objectives\nMain business question\nCase detailed questions\n\nWhat is your hypothesis about factors predicting “ZBC”\nWhat explains loan volume variations across years (e.g. 2000-2005)\n\nMapping to Ravi’s coure week: 2\nOther notes: Divide into basic; geo; unsupervised\n\n\n\nCase C: Estimate Loan Profit/Loss to-Date\n\nLearnig objectives\nMain business question\nCase detailed questions\nMapping to Ravi’s coure week: 2\nOther notes\n\n\n\nCase D1: Classifiy Loan Outcome at Origination\n\nLearnig objectives\nMain business question\nCase detailed questions\nMapping to Ravi’s coure week: 3 (Classification Week)\nOther notes\n\n\n\nCase D2: Classifiy Loan Outcome at Interim Dates\n\nLearnig objectives\nMain business question\nCase detailed questions\nMapping to Ravi’s coure week: 3 (Classification Week)\nOther notes\n\n\n\nCase E1: Predict Loan Profit/Loss at Origination\n\nLearnig objectives\nMain business question\nCase detailed questions\nMapping to Ravi’s coure week: 4 (Regression Week)\nOther notes\n\n\n\nCase E2: Predict Loan Profit/Loss at Interim Dates\n\nLearnig objectives\nMain business question\nCase detailed questions\nMapping to Ravi’s coure week: 4 (Regression Week)\nOther notes\n\n\n\nCase F: Assess Validity of LLPA Matrix\n\nLearnig objectives\nMain business question\nCase detailed questions\nMapping to Ravi’s coure week: 5 (Prescriptive Week)\nOther notes\n\n\n\nCase G: Design Your LLPA Matrix\n\nLearnig objectives\nMain business question\nCase detailed questions\nMapping to Ravi’s coure week: 5 (Prescriptive Week)\nOther notes\n\n\n\nCase H: Examine Georgaphical Variations\n\nLearnig objectives\nMain business question\nCase detailed questions\nMapping to Ravi’s coure week\nOther notes\n\n\n\nCase I: Prescriptive Analytics\n\nLearnig objectives\nMain business question\nCase detailed questions\nMapping to Ravi’s coure week\nOther notes"
  },
  {
    "objectID": "draft_outline.html#iii.-resources",
    "href": "draft_outline.html#iii.-resources",
    "title": "Case Outline",
    "section": "III. Resources",
    "text": "III. Resources\n\nResource Collection 1: FNMA Public Resources\n\n\nResource Collection 2: Python Resources"
  },
  {
    "objectID": "draft_outline.html#iv.-synthetic-datasets",
    "href": "draft_outline.html#iv.-synthetic-datasets",
    "title": "Case Outline",
    "section": "IV. Synthetic Datasets",
    "text": "IV. Synthetic Datasets"
  },
  {
    "objectID": "draft_outline.html#v.-instructor-resources",
    "href": "draft_outline.html#v.-instructor-resources",
    "title": "Case Outline",
    "section": "V. Instructor Resources",
    "text": "V. Instructor Resources"
  },
  {
    "objectID": "expository_mortgages.html",
    "href": "expository_mortgages.html",
    "title": "Mortgages",
    "section": "",
    "text": "In the United States, a mortgage is a loan issued by a lender (typically a bank or financial institution) to a borrower (typically a prospective or current homeowner) for the purposes of financing the purchase of a real property (e.g. a single-family house), refinancing an existing purchase (e.g. replacing an existing mortgage) or, more generally, raising funds using the value of the property as collateral. A mortgage is backed by the value of the real property; that is, the lender has a lien1 on the property until the loan is paid in full. If the lender fails to pay the sequence of payments stipulated by the mortgage contract, or is severely delinquent in these payments, then the lender has a right to liquidate the property to recover the unpaid loan amount."
  },
  {
    "objectID": "expository_mortgages.html#footnotes",
    "href": "expository_mortgages.html#footnotes",
    "title": "Mortgages",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nan interest over the property to secure the repayment of the loan↩︎"
  },
  {
    "objectID": "expository_note_3.html",
    "href": "expository_note_3.html",
    "title": "C. Examining the Porfitability of Zero Balance Loans",
    "section": "",
    "text": "Test here."
  },
  {
    "objectID": "expository_overview.html",
    "href": "expository_overview.html",
    "title": "Overview",
    "section": "",
    "text": "See side bar for an evolving list of helpful expository notes."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "This case is intended to provide Business Analytics Masters students with a real-life, real-data, setting to implement the various stages of the Analytics Value Chain: Expoloratory, Predictive, and (coming soon), Prescriptive. The dataset offers the hallmarks and challenges of handling large data with various practically important but subtle considerations.\nThe case format is novel. It’s an online site which allows dynamic modifications. An evolving set of Exploratory notes provide background information (domain and methodology) and help guide students in their analysis. We believe the format better reflects the experiences of data scientists and business analytsts is practice. The are confronted by a semistructured patchwork of data, are asked to assume responsibility for various deliverables, and jave access to only a patchwork of problem context and background information.\n\n\n\n\n\n\nNote\n\n\n\nThis case site is under construction. Content is evolving! Users are encouraged to contact the authors for feedback and suggestions!\n\n\nPlease note the this case is intended for teaching purposes only and does not serve as endorsement of investment or management approaches.\nBest wishes,\nA. Farahat and R. Ravi"
  },
  {
    "objectID": "modules_overview.html",
    "href": "modules_overview.html",
    "title": "Overview",
    "section": "",
    "text": "See sidebar for various case modules."
  },
  {
    "objectID": "module_cohort_outcome_origination.html",
    "href": "module_cohort_outcome_origination.html",
    "title": "Cohort-level Outcome Prediction at Origination",
    "section": "",
    "text": "Learnig objectives\nMain business question\nCase detailed questions\nMapping to Ravi’s coure week: 4 (Regression Week)\nOther notes"
  },
  {
    "objectID": "module_data_prep.html",
    "href": "module_data_prep.html",
    "title": "Data Pre-Processing: Loan-Level Consolidation",
    "section": "",
    "text": "Task outline\n\nDownload from Fannie Mae’s Data Dynamics portal the single-family historical loan performance datasets corresponding to loans acquired in the most recent years (say 2018-2022). For more extensive analysis, download all the datasets starting year 2000. (Warning: The datasets are large). We recommend you first examine the Loan Performance Data Tutorial, the Sample File, and CRT Glossary and File Layout all available here.\nFilter the data to consider only on 30 year mortgages (the data already excludes adjustable rate and other less common mortgage types)\nIdentify the static fields (those related to acquisition characteristics and unlikley to change my reporting period), the dynamic fileds (those likely to change by reporting period), and non-applicable fields that do not apply to fixed-rate mortgages.\nConsolidate (aggregate) the data by Loan Identifier to create a single record per loan. In particular, extract the the first (available) data for static fields and the last available data for dynamic fields for each loan. Drop all non-applicable fields.\nSave the consolidated files by acquisition year.\nThe year Fannie Mae acquired a loan may differ from the year the loan originated. For our purposes, it is more meaningful to conduct analysis by loan vintage (origination year). Therefore, you nneed to re-read the files saved in Step 5, split and re-combine them by acquisition year, and save the resuting files."
  },
  {
    "objectID": "module_geo_variations.html",
    "href": "module_geo_variations.html",
    "title": "Exploring Geographical Variations",
    "section": "",
    "text": "Learnig objectives\nMain business question\nCase detailed questions\n\nWhat is your hypothesis about factors predicting “ZBC”\nWhat explains loan volume variations across years (e.g. 2000-2005)\n\nMapping to Ravi’s coure week: 2\nOther notes: Divide into basic; geo; unsupervised"
  },
  {
    "objectID": "module_loan_outcome_origination.html",
    "href": "module_loan_outcome_origination.html",
    "title": "Predictive Analytics: Predicting Loan Outcomes Based on Origination Characteristics",
    "section": "",
    "text": "Mortgage borrowers have the option of early prepayment (inducing interest rate risk for lenders) and the option of default (inducing credit risk for lenders). Therefore, lenders, financial institutions, and investors seek high quaity data analysis that can help them predict the probabilities of the loan outcomes. Understanding the drivers and magnitudes of these risks helps lenders and investors determine criteria for conforming loans, price these loans to reflect their inherest risks, and to accurately value mortgage-backed securities.\n\nTask outline\n\nDownload the train / test sets described in this section.\nPrepare your dataset: Define a new target variable column named “ZBC_status” and compute it as follows:\n\nSet “ZBC_status” to “Prepaid” if “Zero Balance Code” equals “01”;\nSet “ZBC_status” to “Defaulted” if “Zero Balance Code” equals “02”, “03”, “09”, or “15”;\nSet “ZBC_status” to “Other” if “Zero Balance Code” equals “06”, “16”, “96”;\nSet “ZBC_status” to “Current”, otherwise.\n\nInspect the train dataset. In particular, examine: - The extent and pattern of missing (NA) values; - The value counts of records belonging to each ZBC_status category.\nPlan your analysis (carefully):\n\nIdentify the main challenges involved in classifying loan outcomes based on the dataset you constructed and the predictive variables you have identified above;\nDescribe how you tackle these challenges in your analysis. What method (or sequence of methods) will you implement. Assume you have only a couple of hours to produce a “first-iteration” predictive model;\nHow do you plan to evaluate the quality of your predictive model. Be precise.\n\nExecute (Run) and review your analysis:\n\nConstruct and run your analysis in a notebook. Try running different ML models and compare their test set predictive performance.\nExamine any unexpected results or anomalies. Revise your analysis as necessary.\n\nReport initial findings:\n\nProduce one or more exhibits summarizing your main findings. What is the bets fitting model. WHat is its performance on the test set (based on your selected metric)\nReport on any shortcomings / limitation of your model in predicting the outcome of loans originating next month. WHat ideas, if any, do you have for improving your analysis?"
  },
  {
    "objectID": "module_performance_summary.html",
    "href": "module_performance_summary.html",
    "title": "Performance To-Date Summary Statistics",
    "section": "",
    "text": "Learnig objectives\nMain business question\nCase detailed questions\n\nWhat is your hypothesis about factors predicting “ZBC”\nWhat explains loan volume variations across years (e.g. 2000-2005)\n\nMapping to Ravi’s coure week: 2\nOther notes: Divide into basic; geo; unsupervised"
  },
  {
    "objectID": "MSBA_instructor_resources.html",
    "href": "MSBA_instructor_resources.html",
    "title": "Instructor Resources",
    "section": "",
    "text": "Text goes here."
  }
]